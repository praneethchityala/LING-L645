{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2-DyJHukPJ8g"
      },
      "outputs": [],
      "source": [
        "import sys, re\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "def preprocess(s):\n",
        "    \"\"\"Tokenise a line\"\"\"\n",
        "    o = re.sub('([^a-zA-Z0-9\\']+)', ' \\g<1> ', s.strip())\n",
        "    return ['<BOS>','<BOS>'] + re.sub('  *', ' ', o).strip().split(' ')\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "EMBEDDING_DIM = 4\n",
        "CONTEXT_SIZE = 2 #!!!#\n",
        "HIDDEN_DIM = 6\n",
        "\n",
        "# Bigram Neural Network Model\n",
        "class BigramNNmodel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, context_size, hidden_dim):\n",
        "        super(BigramNNmodel, self).__init__()\n",
        "        self.context_size = context_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear1 = nn.Linear(context_size * embedding_dim, hidden_dim)\n",
        "        self.linear2 = nn.Linear(hidden_dim, vocab_size, bias = False)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # compute x': concatenation of x1 and x2 embeddings\n",
        "        embeds = self.embeddings(inputs).view(\n",
        "                (-1,self.context_size * self.embedding_dim))\n",
        "        # compute h: tanh(W_1.x' + b)\n",
        "        out = torch.tanh(self.linear1(embeds))\n",
        "        # compute W_2.h\n",
        "        out = self.linear2(out)\n",
        "        # compute y: log_softmax(W_2.h)\n",
        "        log_probs = F.log_softmax(out, dim=1)\n",
        "        # return log probabilities\n",
        "        # BATCH_SIZE x len(vocab)\n",
        "        return log_probs\n",
        "\n",
        "# Trigram Neural Network Model\n",
        "class TrigramNNmodel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, context_size, hidden_dim):\n",
        "        super(TrigramNNmodel, self).__init__()\n",
        "        self.context_size = context_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear1 = nn.Linear(context_size * embedding_dim, hidden_dim)\n",
        "        self.linear2 = nn.Linear(hidden_dim, vocab_size, bias = False)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # compute x': concatenation of x1 and x2 embeddings\n",
        "        embeds = self.embeddings(inputs).view(\n",
        "                (-1,self.context_size * self.embedding_dim))\n",
        "        # compute h: tanh(W_1.x' + b)\n",
        "        out = torch.tanh(self.linear1(embeds))\n",
        "        # compute W_2.h\n",
        "        out = self.linear2(out)\n",
        "        # compute y: log_softmax(W_2.h)\n",
        "        log_probs = F.log_softmax(out, dim=1)\n",
        "        # return log probabilities\n",
        "        # BATCH_SIZE x len(vocab)\n",
        "        return log_probs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bigram Model"
      ],
      "metadata": {
        "id": "DtfgoN_b35mB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, re\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "# from model import *\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "training_samples = []\n",
        "vocabulary = set(['<UNK>'])\n",
        "\n",
        "f = open(\"/content/train.txt\", \"r\")\n",
        "train = f.read().split('\\n')\n",
        "\n",
        "\n",
        "for line in train:\n",
        "    print(line)\n",
        "    tokens = preprocess(line)\n",
        "    for i in tokens: \n",
        "      vocabulary.add(i) \n",
        "    training_samples.append(tokens)\n",
        "    # line = sys.stdin.readline()\n",
        "\n",
        "word2idx = {k: v for v, k in enumerate(vocabulary)}\n",
        "idx2word = {v: k for k, v in word2idx.items()}\n",
        "\n",
        "x_train = []\n",
        "y_train = []\n",
        "for tokens in training_samples:\n",
        "    for i in range(len(tokens) - 2): #!!!#\n",
        "        x_train.append([word2idx[tokens[i]],word2idx[tokens[i+1]]]) #!!!#\n",
        "        y_train.append([word2idx[tokens[i+2]]]) #!!!#\n",
        "\n",
        "x_train = np.array(x_train)\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "BATCH_SIZE = 1\n",
        "NUM_EPOCHS = 10\n",
        "\n",
        "train_set = np.concatenate((x_train, y_train), axis=1)\n",
        "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE)\n",
        "\n",
        "loss_function = nn.NLLLoss()\n",
        "model = BigramNNmodel(len(vocabulary), EMBEDDING_DIM, CONTEXT_SIZE, HIDDEN_DIM)\n",
        "optimiser = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    for i, data_tensor in enumerate(train_loader):\n",
        "        context_tensor = data_tensor[:,0:2] #!!!#\n",
        "        target_tensor = data_tensor[:,2] #!!!#\n",
        "\n",
        "        model.zero_grad()\n",
        "\n",
        "        log_probs = model(context_tensor)\n",
        "        loss = loss_function(log_probs, target_tensor)\n",
        "\n",
        "        loss.backward()\n",
        "        optimiser.step()    \n",
        "\n",
        "    print('Epoch:', epoch, 'loss:', float(loss))\n",
        "\n",
        "torch.save({'model': model.state_dict(), 'vocab': idx2word}, 'bigram.lm')\n",
        "\n",
        "print('Model saved.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U68c3-t-3SeK",
        "outputId": "61b9c269-0297-4a16-9a5c-6f4bff24232d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "are you still here ?\n",
            "where are you ?\n",
            "are you tired ?\n",
            "i am tired .\n",
            "are you in england ?\n",
            "were you in mexico ?\n",
            "Epoch: 0 loss: 2.399266004562378\n",
            "Epoch: 1 loss: 2.140509605407715\n",
            "Epoch: 2 loss: 1.8559238910675049\n",
            "Epoch: 3 loss: 1.531304121017456\n",
            "Epoch: 4 loss: 1.1923426389694214\n",
            "Epoch: 5 loss: 0.8930347561836243\n",
            "Epoch: 6 loss: 0.6651957035064697\n",
            "Epoch: 7 loss: 0.5060691833496094\n",
            "Epoch: 8 loss: 0.3975469470024109\n",
            "Epoch: 9 loss: 0.32211777567863464\n",
            "Model saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bigram Testing"
      ],
      "metadata": {
        "id": "uYxPNNyp3_Kx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, re\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "# from feedforward.model import *\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "blob = torch.load('/content/bigram.lm')\n",
        "idx2word = blob['vocab']\n",
        "word2idx = {k: v for v, k in idx2word.items()}\n",
        "vocabulary = set(idx2word.values())\n",
        "\n",
        "model = BigramNNmodel(len(vocabulary), EMBEDDING_DIM, CONTEXT_SIZE, HIDDEN_DIM)\n",
        "model.load_state_dict(blob['model'])\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "BATCH_SIZE = 1\n",
        "\n",
        "f = open(\"/content/test.txt\", \"r\")\n",
        "test = f.read().split('\\n')\n",
        "# print(line)\n",
        "# train = sys.stdin.readline()\n",
        "for line in test:\n",
        "    tokens = preprocess(line)\n",
        "    \n",
        "    x_test = []\n",
        "    y_test = []\n",
        "    for i in range(len(tokens) - 2): #!!!#\n",
        "        x_test.append([word2idx[tokens[i]],word2idx[tokens[i+1]]]) #!!!#\n",
        "        y_test.append([word2idx[tokens[i+2]]]) #!!!#\n",
        "    \n",
        "    x_test = np.array(x_test)\n",
        "    y_test = np.array(y_test)\n",
        "    \n",
        "    test_set = np.concatenate((x_test, y_test), axis=1)\n",
        "    test_loader = DataLoader(test_set, batch_size=BATCH_SIZE)\n",
        "    \n",
        "    total_prob = 1.0\n",
        "    for i, data_tensor in enumerate(test_loader):\n",
        "        context_tensor = data_tensor[:,0:2] #!!!#\n",
        "        target_tensor = data_tensor[:,2] #!!!#\n",
        "        log_probs = model(context_tensor)\n",
        "        probs = torch.exp(log_probs)\n",
        "        predicted_label = int(torch.argmax(probs, dim=1)[0])\n",
        "    \n",
        "        true_label = y_test[i][0]\n",
        "        true_word = idx2word[true_label]\n",
        "    \n",
        "        prob_true = float(probs[0][true_label])\n",
        "        total_prob *= prob_true\n",
        "    \n",
        "    print('%.6f\\t%.6f\\t' % (total_prob, math.log(total_prob)), tokens)\n",
        "    \n",
        "    # line = sys.stdin.readline()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5UNPkxUk3gTk",
        "outputId": "5bbaeb17-5e5d-421e-abfd-ccbb89d02294"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.001799\t-6.320797\t ['<BOS>', '<BOS>', 'where', 'are', 'you', '?']\n",
            "0.006107\t-5.098277\t ['<BOS>', '<BOS>', 'were', 'you', 'in', 'england', '?']\n",
            "0.026388\t-3.634832\t ['<BOS>', '<BOS>', 'are', 'you', 'in', 'mexico', '?']\n",
            "0.000013\t-11.245216\t ['<BOS>', '<BOS>', 'i', 'am', 'in', 'mexico', '.']\n",
            "0.000119\t-9.039762\t ['<BOS>', '<BOS>', 'are', 'you', 'still', 'in', 'mexico', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trigram Model"
      ],
      "metadata": {
        "id": "jo4wrEdb4DRi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, re\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "# from model import *\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "training_samples = []\n",
        "vocabulary = set(['<UNK>'])\n",
        "\n",
        "f = open(\"/content/train.txt\", \"r\")\n",
        "train = f.read().split('\\n')\n",
        "\n",
        "\n",
        "for line in train:\n",
        "    print(line)\n",
        "    tokens = preprocess(line)\n",
        "    for i in tokens: \n",
        "      vocabulary.add(i) \n",
        "    training_samples.append(tokens)\n",
        "    # line = sys.stdin.readline()\n",
        "\n",
        "word2idx = {k: v for v, k in enumerate(vocabulary)}\n",
        "idx2word = {v: k for k, v in word2idx.items()}\n",
        "\n",
        "x_train = []\n",
        "y_train = []\n",
        "for tokens in training_samples:\n",
        "    for i in range(len(tokens) - 2): #!!!#\n",
        "        x_train.append([word2idx[tokens[i]],word2idx[tokens[i+1]]]) #!!!#\n",
        "        y_train.append([word2idx[tokens[i+2]]]) #!!!#\n",
        "\n",
        "x_train = np.array(x_train)\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "BATCH_SIZE = 1\n",
        "NUM_EPOCHS = 10\n",
        "\n",
        "train_set = np.concatenate((x_train, y_train), axis=1)\n",
        "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE)\n",
        "\n",
        "loss_function = nn.NLLLoss()\n",
        "model = TrigramNNmodel(len(vocabulary), EMBEDDING_DIM, CONTEXT_SIZE, HIDDEN_DIM)\n",
        "optimiser = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    for i, data_tensor in enumerate(train_loader):\n",
        "        context_tensor = data_tensor[:,0:2] #!!!#\n",
        "        target_tensor = data_tensor[:,2] #!!!#\n",
        "\n",
        "        model.zero_grad()\n",
        "\n",
        "        log_probs = model(context_tensor)\n",
        "        loss = loss_function(log_probs, target_tensor)\n",
        "\n",
        "        loss.backward()\n",
        "        optimiser.step()    \n",
        "\n",
        "    print('Epoch:', epoch, 'loss:', float(loss))\n",
        "\n",
        "torch.save({'model': model.state_dict(), 'vocab': idx2word}, 'trigram.lm')\n",
        "\n",
        "print('Model saved.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRbFHlhIPPbg",
        "outputId": "7ec746c5-96fd-423a-938d-6d03b94ff463"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "are you still here ?\n",
            "where are you ?\n",
            "are you tired ?\n",
            "i am tired .\n",
            "are you in england ?\n",
            "were you in mexico ?\n",
            "Epoch: 0 loss: 2.6104564666748047\n",
            "Epoch: 1 loss: 2.2813572883605957\n",
            "Epoch: 2 loss: 1.9093862771987915\n",
            "Epoch: 3 loss: 1.5117557048797607\n",
            "Epoch: 4 loss: 1.1313836574554443\n",
            "Epoch: 5 loss: 0.8182763457298279\n",
            "Epoch: 6 loss: 0.5949320197105408\n",
            "Epoch: 7 loss: 0.44837528467178345\n",
            "Epoch: 8 loss: 0.3548448979854584\n",
            "Epoch: 9 loss: 0.2945851981639862\n",
            "Model saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trigram testing"
      ],
      "metadata": {
        "id": "3xwvSPKc4G51"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, re\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "# from feedforward.model import *\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "blob = torch.load('/content/trigram.lm')\n",
        "idx2word = blob['vocab']\n",
        "word2idx = {k: v for v, k in idx2word.items()}\n",
        "vocabulary = set(idx2word.values())\n",
        "\n",
        "model = BigramNNmodel(len(vocabulary), EMBEDDING_DIM, CONTEXT_SIZE, HIDDEN_DIM)\n",
        "model.load_state_dict(blob['model'])\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "BATCH_SIZE = 1\n",
        "\n",
        "f = open(\"/content/test.txt\", \"r\")\n",
        "test = f.read().split('\\n')\n",
        "# print(line)\n",
        "# train = sys.stdin.readline()\n",
        "for line in test:\n",
        "    tokens = preprocess(line)\n",
        "    \n",
        "    x_test = []\n",
        "    y_test = []\n",
        "    for i in range(len(tokens) - 2): #!!!#\n",
        "        x_test.append([word2idx[tokens[i]],word2idx[tokens[i+1]]]) #!!!#\n",
        "        y_test.append([word2idx[tokens[i+2]]]) #!!!#\n",
        "    \n",
        "    x_test = np.array(x_test)\n",
        "    y_test = np.array(y_test)\n",
        "    \n",
        "    test_set = np.concatenate((x_test, y_test), axis=1)\n",
        "    test_loader = DataLoader(test_set, batch_size=BATCH_SIZE)\n",
        "    \n",
        "    total_prob = 1.0\n",
        "    for i, data_tensor in enumerate(test_loader):\n",
        "        context_tensor = data_tensor[:,0:2] #!!!#\n",
        "        target_tensor = data_tensor[:,2] #!!!#\n",
        "        log_probs = model(context_tensor)\n",
        "        probs = torch.exp(log_probs)\n",
        "        predicted_label = int(torch.argmax(probs, dim=1)[0])\n",
        "    \n",
        "        true_label = y_test[i][0]\n",
        "        true_word = idx2word[true_label]\n",
        "    \n",
        "        prob_true = float(probs[0][true_label])\n",
        "        total_prob *= prob_true\n",
        "    \n",
        "    print('%.6f\\t%.6f\\t' % (total_prob, math.log(total_prob)), tokens)\n",
        "    \n",
        "    # line = sys.stdin.readline()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cD2CDm3kS72h",
        "outputId": "778a2614-bfdf-48f5-adb1-15eed5c6920d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.004271\t-5.455930\t ['<BOS>', '<BOS>', 'where', 'are', 'you', '?']\n",
            "0.005562\t-5.191857\t ['<BOS>', '<BOS>', 'were', 'you', 'in', 'england', '?']\n",
            "0.019459\t-3.939468\t ['<BOS>', '<BOS>', 'are', 'you', 'in', 'mexico', '?']\n",
            "0.000005\t-12.184215\t ['<BOS>', '<BOS>', 'i', 'am', 'in', 'mexico', '.']\n",
            "0.000044\t-10.040571\t ['<BOS>', '<BOS>', 'are', 'you', 'still', 'in', 'mexico', '?']\n"
          ]
        }
      ]
    }
  ]
}